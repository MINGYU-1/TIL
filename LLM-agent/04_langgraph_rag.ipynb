{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c4bb01",
   "metadata": {},
   "source": [
    "# RAG w/ Langgraph\n",
    "`12_langgraph_rag.ipynb`\n",
    "\n",
    "- https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c66afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a85d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\mingyu\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mingyu\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mingyu\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751b5d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 1. Loader (웹문서)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import SoupStrainer          # 권장\n",
    "# 또는\n",
    "from bs4.element import SoupStrainer\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    # 문서 출처 URL\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/', ),\n",
    "    # 웹페이지 안에서 필요한 정보만 선택\n",
    "    bs_kwargs={\n",
    "        'parse_only': SoupStrainer(class_=['post-content']) \n",
    "    }\n",
    "    # header_template={}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(len(splitted_docs))\n",
    "\n",
    "# 3. Embedding Model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-small')  # small <-> large\n",
    "\n",
    "# 4. Vectorstore (지금은 FAISS -> 클라우드-Pinecone)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(splitted_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd81a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "for m in prompt.messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25caa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "\n",
    "# State\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Node\n",
    "# 검색 노드\n",
    "def retrieve(state: State):\n",
    "    # [ Document * 4 ]\n",
    "    retrieved_docs = vectorstore.similarity_search(state['question'], k=4)\n",
    "    \n",
    "    # Document 객체의 필요없는 정보는 다 빼고, 내용에 해당하는 page_content 만 모아서 넘기면 토큰 절약 가능.\n",
    "    # context_str = ''\n",
    "    # for doc in retrieved_docs:\n",
    "    #     context_str += doc.page_content + '\\n------------------------\\n'\n",
    "\n",
    "    # 나머지 return 하지 않은 state 항목들은, 알아서 그대로 감 (question, answer 는 알아서 그대로 나감)\n",
    "    return { 'context': context_str, }\n",
    "\n",
    "# 답변 생성노드\n",
    "def generate(state: State):\n",
    "    context_str = ''\n",
    "    for doc in state['context']:\n",
    "        context_str += doc.page_content + '\\n-------------------\\n'\n",
    "    question_with_context = prompt.invoke({'question': state['question'], 'context': state['context']})\n",
    "    response = llm.invoke(question_with_context)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "# Graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('generate', generate)\n",
    "\n",
    "builder.add_edge(START, 'retrieve')\n",
    "builder.add_edge('retrieve', 'generate')\n",
    "builder.add_edge('generate', END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 출력\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d73867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 타입 예시\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: str   # retrieve에서 채워줄 키\n",
    "    answer: str    # generate에서 채울 키\n",
    "\n",
    "# retrieve 노드\n",
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.invoke(question)  # 또는 retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 컨텐츠만 이어붙여 문자열 컨텍스트 생성\n",
    "    context_str = \"\\n\\n\".join(d.page_content for d in docs) if docs else \"\"\n",
    "\n",
    "    # 여기서 반드시 context_str를 반환 키와 맞춰 사용\n",
    "    return {\"context\": context_str}\n",
    "\n",
    "# generate 노드(참고)\n",
    "def generate(state: State):\n",
    "    prompt = f\"질문:\\n{state['question']}\\n\\n관련 문맥:\\n{state['context']}\\n\\n정확하고 간결히 답하세요.\"\n",
    "    resp = llm.invoke(prompt)  # 사용 중인 LLM 호출로 대체\n",
    "    return {\"answer\": resp.content if hasattr(resp, \"content\") else str(resp)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a4b6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"generate\", generate)\n",
    "\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2978fa",
   "metadata": {},
   "source": [
    "*메세지 스트리밍*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1562413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "\n",
    "# State\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Node\n",
    "# 검색 노드\n",
    "def retrieve(state: State):\n",
    "    # [ Document * 4 ]\n",
    "    retrieved_docs = vectorstore.similarity_search(state['question'], k=4)\n",
    "    \n",
    "    # Document 객체의 필요없는 정보는 다 빼고, 내용에 해당하는 page_content 만 모아서 넘기면 토큰 절약 가능.\n",
    "    context_str = ''\n",
    "    for doc in retrieved_docs:\n",
    "        context_str += doc.page_content + '\\n------------------------\\n'\n",
    "\n",
    "    # 나머지 return 하지 않은 state 항목들은, 알아서 그대로 감 (question, answer 는 알아서 그대로 나감)\n",
    "    return { 'context': context_str, }\n",
    "\n",
    "# 답변 생성노드\n",
    "def generate(state: State):\n",
    "    question_with_context = prompt.invoke({'question': state['question'], 'context': state['context']})\n",
    "    response = llm.invoke(question_with_context)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "# Graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('generate', generate)\n",
    "\n",
    "builder.add_edge(START, 'retrieve')\n",
    "builder.add_edge('retrieve', 'generate')\n",
    "builder.add_edge('generate', END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 출력\n",
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295cfb0",
   "metadata": {},
   "source": [
    "## RAG + \n",
    "- Metadata 편집\n",
    "-Query 분석 - 보완\n",
    "-사용자 질문을 재분배해야 한다. 다른 문서가 나올 수 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75c35170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 63개중 1/3 지점\n",
    "third = len(splitted_docs) // 3\n",
    "\n",
    "# metadata에 'section' 추가중\n",
    "for idx, doc in enumerate(splitted_docs):\n",
    "    if idx < third:\n",
    "        doc.metadata['section'] = '전반부'\n",
    "    elif idx < third *2:\n",
    "        doc.metadata['section'] = '중반부'\n",
    "    else:\n",
    "        doc.metadata['section'] = '후반부'\n",
    "\n",
    "splitted_docs[0].metadata\n",
    "vectorstore = FAISS.from_documents(splitted_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee71e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State를 더 빡빡하게 정의하기 위해\n",
    "# StructuredOutPut\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated \n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Vectorstore Search Query\"\"\"\n",
    "    # 1. 타입, 2. NOT NULL, 3. 설명\n",
    "    query: Annotated[str, ...,'Search query to run']\n",
    "    section: Annotated [\n",
    "        Literal['전반부','중반부','후반부'],\n",
    "        ...,\n",
    "        'Section to query'\n",
    "    ]\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context:List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node\n",
    "def analyze_query(state: MyState):\n",
    "    #Search 클래스에 맞춰 사용자 question을 {query, section}으로 바꿈\n",
    "    s_llm = llm.with_structured_output(Search)\n",
    "    query = s_llm.invoke(state['question'])\n",
    "    print(query)    \n",
    "    return {'query':query}\n",
    "def retrieve(state: MyState):\n",
    "    query = state['query']\n",
    "    docs = vectorstore.similarity_search(\n",
    "        query['query'],\n",
    "        # llm이 판단한 section과 실제 문서조각의 section이 맞을 경우에만 검색.\n",
    "        filter = lambda doc: doc.metadata.get('section') == query['section']\n",
    "    )\n",
    "    return {'context':docs}\n",
    "def generate(state: MyState):\n",
    "    # Token 아끼기 위해, 내용만 추려서 문자열로 만들기\n",
    "    doc_str = ''\n",
    "    for doc in state['context']:\n",
    "        doc_str += doc.page_content + '\\n======================\\n'\n",
    "    question_with_context = prompt.invoke({'question':state['question'],'context':doc_str})\n",
    "    res = llm.invoke(question_with_context)\n",
    "    return {'answer':res.content}\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node('analye_query',analyze_query)\n",
    "builder.add_node('retrieve',retrieve)\n",
    "builder.add_node('generate',generate)\n",
    "\n",
    "builder.add_edge(START, 'analyze_query')\n",
    "builder.add_edge('analyze_query','retrieve')\n",
    "builder.add_edge('retrieve','generate')\n",
    "builder.add_edge('generate',)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
